{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYv4owKe+hrzuPWe58pr8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ragboom7/EPL-matches-predictor/blob/main/Webscrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfXkey1a_pGQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
      ],
      "metadata": {
        "id": "7FdGcGEdA7Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = requests.get(standings_url)"
      ],
      "metadata": {
        "id": "Tpf1MsHzBAJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(data.text)\n",
        "standings_table = soup.select('table.stats_table')[0]\n",
        "links = standings_table.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if '/squads/' in l]"
      ],
      "metadata": {
        "id": "MOETlgmkB82M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "data = requests.get(team_urls[0])"
      ],
      "metadata": {
        "id": "yNif75yMCBXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]"
      ],
      "metadata": {
        "id": "CCQ23IqaCD6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'all_comps/shooting/' in l]"
      ],
      "metadata": {
        "id": "wWzOdkKqCaPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "shooting = pd.read_html(data.text, match=\"Shooting\")[0]"
      ],
      "metadata": {
        "id": "mDj8Yby6CcUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shooting.columns = shooting.columns.droplevel()\n",
        "team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")"
      ],
      "metadata": {
        "id": "uUdkOpqQCet0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years = list(range(2024, 2017, -1))\n",
        "bro = []"
      ],
      "metadata": {
        "id": "7ynWx_lmCii3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
      ],
      "metadata": {
        "id": "eyGGCtvxCquc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "for year in years:\n",
        "    data = requests.get(standings_url)\n",
        "    soup = BeautifulSoup(data.text)\n",
        "    standings_table = soup.select('table.stats_table')[0]\n",
        "\n",
        "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
        "    links = [l for l in links if '/squads/' in l]\n",
        "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "\n",
        "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
        "    standings_url = f\"https://fbref.com{previous_season}\"\n",
        "    for team_url in team_urls:\n",
        "      team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
        "      data = requests.get(team_url)\n",
        "      matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
        "      soup = BeautifulSoup(data.text)\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
        "      shooting.columns = shooting.columns.droplevel()\n",
        "      team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\",\"SoT%\",\"G/Sh\",\"G/SoT\"]], on=\"Date\")\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/keeper/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      keeping = pd.read_html(data.text, match=\"Goalkeeping\")[0]\n",
        "      keeping.columns = keeping.columns.droplevel()\n",
        "      team_data1 = team_data.merge(keeping[[\"Date\", \"SoTA\", \"Saves\", \"Save%\",\"CS\",\"PSxG\"]], on=\"Date\")\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/passing/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      passing = pd.read_html(data.text, match=\"Passing\")[0]\n",
        "      passing.columns = passing.columns.droplevel()\n",
        "      team_data2 = team_data1.merge(passing[[\"Date\", \"Cmp\", \"Att\", \"Cmp%\",\"TotDist\",\"PrgDist\",\"KP\",\"PPA\",\"PrgP\"]], on=\"Date\")\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/defense/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      defense = pd.read_html(data.text, match=\"Defensive Actions\")[0]\n",
        "      defense.columns = defense.columns.droplevel()\n",
        "      team_data3 = team_data2.merge(defense[[\"Date\", \"Tkl\", \"TklW\", \"Blocks\",\"Sh\",\"Pass\",\"Int\",\"Err\",\"Tkl+Int\"]], on=\"Date\")\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/possession/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      poss = pd.read_html(data.text, match=\"Possession\")[0]\n",
        "      poss.columns = poss.columns.droplevel()\n",
        "      team_data4 = team_data3.merge(poss[[\"Date\", \"Touches\", \"Att Pen\", \"Att\",\"Carries\",\"PrgDist\",\"CPA\",\"Rec\",\"PrgR\"]], on=\"Date\")\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/misc/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      misc = pd.read_html(data.text, match=\"Miscellaneous Stats\")[0]\n",
        "      misc.columns = misc.columns.droplevel()\n",
        "      team_data5 = team_data4.merge(misc[[\"Date\", \"Fls\",\"Fld\",\"Crs\"]], on=\"Date\")\n",
        "      team_data = team_data5[team_data5[\"Comp\"] == \"Premier League\"]\n",
        "      team_data.loc[team_data['Date'].str.startswith('2025'), 'Season'] = 2025\n",
        "      team_data.loc[team_data['Date'].str.startswith('2024'), 'Season'] = 2024\n",
        "      team_data['Team']=team_name\n",
        "      bro.append(team_data)\n",
        "      time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "U0c7Skn4C4MS",
        "outputId": "8c0a90b6-8670-450e-ec39-8f68f9a378e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport time\\nfor year in years:\\n    data = requests.get(standings_url)\\n    soup = BeautifulSoup(data.text)\\n    standings_table = soup.select(\\'table.stats_table\\')[0]\\n\\n    links = [l.get(\"href\") for l in standings_table.find_all(\\'a\\')]\\n    links = [l for l in links if \\'/squads/\\' in l]\\n    team_urls = [f\"https://fbref.com{l}\" for l in links]\\n    \\n    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\\n    standings_url = f\"https://fbref.com{previous_season}\"\\n    for team_url in team_urls:\\n      team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\\n      data = requests.get(team_url)\\n      matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\\n      soup = BeautifulSoup(data.text)\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/shooting/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      shooting = pd.read_html(data.text, match=\"Shooting\")[0]\\n      shooting.columns = shooting.columns.droplevel()\\n      team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\",\"SoT%\",\"G/Sh\",\"G/SoT\"]], on=\"Date\")\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/keeper/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      keeping = pd.read_html(data.text, match=\"Goalkeeping\")[0]\\n      keeping.columns = keeping.columns.droplevel()\\n      team_data1 = team_data.merge(keeping[[\"Date\", \"SoTA\", \"Saves\", \"Save%\",\"CS\",\"PSxG\"]], on=\"Date\")\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/passing/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      passing = pd.read_html(data.text, match=\"Passing\")[0]\\n      passing.columns = passing.columns.droplevel()\\n      team_data2 = team_data1.merge(passing[[\"Date\", \"Cmp\", \"Att\", \"Cmp%\",\"TotDist\",\"PrgDist\",\"KP\",\"PPA\",\"PrgP\"]], on=\"Date\")\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/defense/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      defense = pd.read_html(data.text, match=\"Defensive Actions\")[0]\\n      defense.columns = defense.columns.droplevel()\\n      team_data3 = team_data2.merge(defense[[\"Date\", \"Tkl\", \"TklW\", \"Blocks\",\"Sh\",\"Pass\",\"Int\",\"Err\",\"Tkl+Int\"]], on=\"Date\")\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/possession/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      poss = pd.read_html(data.text, match=\"Possession\")[0]\\n      poss.columns = poss.columns.droplevel()\\n      team_data4 = team_data3.merge(poss[[\"Date\", \"Touches\", \"Att Pen\", \"Att\",\"Carries\",\"PrgDist\",\"CPA\",\"Rec\",\"PrgR\"]], on=\"Date\")\\n      links = [l.get(\"href\") for l in soup.find_all(\\'a\\')]\\n      links = [l for l in links if l and \\'all_comps/misc/\\' in l]\\n      data = requests.get(f\"https://fbref.com{links[0]}\")\\n      misc = pd.read_html(data.text, match=\"Miscellaneous Stats\")[0]\\n      misc.columns = misc.columns.droplevel()\\n      team_data5 = team_data4.merge(misc[[\"Date\", \"Fls\",\"Fld\",\"Crs\"]], on=\"Date\")\\n      team_data = team_data5[team_data5[\"Comp\"] == \"Premier League\"]\\n      team_data.loc[team_data[\\'Date\\'].str.startswith(\\'2025\\'), \\'Season\\'] = 2025\\n      team_data.loc[team_data[\\'Date\\'].str.startswith(\\'2024\\'), \\'Season\\'] = 2024\\n      team_data[\\'Team\\']=team_name\\n      bro.append(team_data)\\n      time.sleep(1)\\n      '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df1 = pd.concat(bro)\n",
        "match_df1.columns = [c.lower() for c in match_df1.columns]"
      ],
      "metadata": {
        "id": "tR4Gx93FDjVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_df1.to_csv(r\"C:\\Users\\ragha\\Downloads\\eplfinal.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download(r\"C:\\Users\\ragha\\Downloads\\eplfinal.csv\")"
      ],
      "metadata": {
        "id": "Rtw1lKDTD8Yk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}